{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI models Fine Tuning\n",
    "\n",
    "In this notebook, we will fine-tune the OpenAI models `Davinci` & `GPT-3.5` on the dataset generated by `GPT-4` described in the notebook [DatasetCreation](a_DatasetCreation.ipynb). We will then evaluate the performance of these models with and without fine-tuning in the following notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- PREPARING NOTEBOOK ---------------------------- #\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Random seed\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# External modules\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# Set global log level\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Define PWD as the current git repository\n",
    "import git\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "pwd = repo.working_dir\n",
    "os.chdir(pwd)\n",
    "\n",
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- LOAD PREVIOUS NOTEBOOKS ------------------------- #\n",
    "import json\n",
    "import __main__\n",
    "import black\n",
    "\n",
    "paths = [\n",
    "    os.path.join(pwd, \"notebooks\", \"text_simplification\", \"a_DatasetCreation.ipynb\"),\n",
    "]\n",
    "\n",
    "# Read notebooks\n",
    "code_dict = {}\n",
    "for path in paths:\n",
    "    code = \"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        temp = json.load(f)\n",
    "\n",
    "    cells = [\n",
    "        cell\n",
    "        for cell in temp[\"cells\"]\n",
    "        if cell[\"cell_type\"] == \"code\"\n",
    "        and len(cell[\"source\"]) > 0\n",
    "        and cell[\"source\"][-1] == \"# import\"\n",
    "    ]\n",
    "    notebook_code = \"\\n\".join(\n",
    "        line\n",
    "        for cell in cells\n",
    "        for line in cell[\"source\"]\n",
    "        if line != \"# import\" and len(line) > 0 and line[0] != \"%\"\n",
    "    )\n",
    "    # Create something like a header\n",
    "    code += f\"# {'-'*76} #\\n\"\n",
    "    code += f\"# {os.path.basename(path).upper():^76} #\\n\"\n",
    "    code += f\"# {'-'*76} #\\n\"\n",
    "    code += notebook_code\n",
    "\n",
    "    # Add \"Module Creation\"\n",
    "    notebook_name = (\n",
    "        os.path.basename(path).replace(\"imported_\", \"\").replace(\".ipynb\", \"\")\n",
    "    )\n",
    "    code += \"\"\"\n",
    "# --------------------------------- IMPORTER --------------------------------- #\n",
    "import types\n",
    "\n",
    "\n",
    "class MyNotebook:\n",
    "    pass\n",
    "\n",
    "\n",
    "NOTEBOOK_NAME = MyNotebook()\n",
    "# Put every function defined in the notebook in the class\n",
    "NOTEBOOK_NAME.__dict__.update(\n",
    "    {\n",
    "        name: obj\n",
    "        for name, obj in locals().items()\n",
    "        if isinstance(obj, (type, types.FunctionType))\n",
    "        if not (name.startswith(\"_\") or name == \"MyNotebook\")\n",
    "    }\n",
    ")\n",
    "    \"\"\".replace(\n",
    "        \"NOTEBOOK_NAME\", notebook_name\n",
    "    )\n",
    "\n",
    "    # Remove empty lines\n",
    "    code = \"\\n\".join([line for line in code.split(\"\\n\") if len(line) > 0])\n",
    "    # Format code\n",
    "    code = black.format_str(code, mode=black.FileMode())\n",
    "\n",
    "    # Write scrach file\n",
    "    path = os.path.join(\n",
    "        pwd, \"scratch\", f\"imported_{os.path.basename(path).replace('ipynb', 'py')}\"\n",
    "    )\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        os.makedirs(os.path.dirname(path))\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(code)\n",
    "    code_dict[path] = code\n",
    "\n",
    "\n",
    "# Mainify code\n",
    "for path, code in code_dict.items():\n",
    "    compiled = compile(code, path, \"exec\")\n",
    "    exec(compiled, __main__.__dict__)\n",
    "\n",
    "# import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création des fichiers JSON\n",
    "\n",
    "Dans un premier temps, nous devons formatter les exemples présents dans notre jeu de données sous la forme d'un fichier json où chaque ligne a le format suivant :\n",
    "1. Pour le modèle `GPT3.5`\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Context\"}, {\"role\": \"user\", \"content\": \"Original\"}, {\"role\": \"assistant\", \"content\": \"Simplified\"}]}\n",
    "```\n",
    "2. Pour le modèle `Davinci`\n",
    "```json\n",
    "{\"prompt\": \"Original\", \"completion\": \"Simplified\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- FORMATTING FUNCTIONS --------------------------- #\n",
    "# Define Context and templates\n",
    "CONTEXT = \"Vous êtes un modèle de langage naturel capable de simplifier des phrases en français. La phrase simplifiée doit avoir un sens aussi proche que possible de la phrase originale, mais elle est d'un niveau inférieur du CECRL et donc plus facile à comprendre. Par exemple, si une phrase est au niveau C1 du CECRL, simplifiez-la en B2. Si elle se situe au niveau B2, simplifiez-la en B1. Si elle se situe au niveau B1, simplifiez-la en A2. Si le niveau A2 est atteint, simplifiez en A1.\"\n",
    "GPT_TEMPLATE = \"\"\"{\"messages\": [{\"role\": \"system\", \"content\": \"CONTEXT\"}, {\"role\": \"user\", \"content\": \"INPUT\"}, {\"role\": \"assistant\", \"content\": \"OUTPUT\"}]}\"\"\"\n",
    "DAVINCI_TEMPLATE = \"\"\"{\"prompt\": \"INPUT\", \"completion\": \"OUTPUT\"}\"\"\"\n",
    "\n",
    "\n",
    "# Define conversation for GPT\n",
    "def create_gpt_conversation(row, training=True):\n",
    "    if training:\n",
    "        difficulty = row[\"index\"] % 5\n",
    "    else:\n",
    "        difficulty = {\"A2\": 0, \"B1\": 1, \"B2\": 2, \"C1\": 3, \"C2\": 4}[row[\"Difficulty\"]]\n",
    "    instruction = f\"\"\"Voici une phrase en français de niveau CECRL {['A2', 'B1', 'B2', 'C1', 'C2'][difficulty]} à simplifier :\\\\n'''{row['Original']}'''\\\\nDonne moi une phrase simplifiée au niveau CECRL {['A1', 'A2', 'B1', 'B2', 'C1'][difficulty]} tout en conservant au maximum son sens original\"\"\"\n",
    "    if training:\n",
    "        return (\n",
    "            GPT_TEMPLATE.replace(\"CONTEXT\", CONTEXT)\n",
    "            .replace(\"INPUT\", instruction)\n",
    "            .replace(\"OUTPUT\", row[\"Simplified\"])\n",
    "        )\n",
    "    else:\n",
    "        # Return only INPUT\n",
    "        return instruction\n",
    "\n",
    "\n",
    "# Define conversation for Davinci\n",
    "def create_davinci_conversation(row, training=True):\n",
    "    if training:\n",
    "        difficulty = row[\"index\"] % 5\n",
    "    else:\n",
    "        difficulty = {\"A2\": 0, \"B1\": 1, \"B2\": 2, \"C1\": 3, \"C2\": 4}[row[\"Difficulty\"]]\n",
    "    instruction = f\"\"\"Voici une phrase en français de niveau CECRL {['A2', 'B1', 'B2', 'C1', 'C2'][difficulty]} à simplifier :\\\\n'''{row['Original']}'''\\\\nDonne moi une phrase simplifiée au niveau CECRL {['A1', 'A2', 'B1', 'B2', 'C1'][difficulty]} tout en conservant au maximum son sens original\"\"\"\n",
    "    if training:\n",
    "        return DAVINCI_TEMPLATE.replace(\"INPUT\", instruction).replace(\n",
    "            \"OUTPUT\", row[\"Simplified\"]\n",
    "        )\n",
    "    else:\n",
    "        # Return INPUT only\n",
    "        return instruction\n",
    "\n",
    "\n",
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353f4c6c4986464a86dd30a49d0bc3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lopilo/code/Lingorank_LLM/scratch/imported_a_DatasetCreation.py:71: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- JSON FILES CREATION --------------------------- #\n",
    "import csv\n",
    "\n",
    "# Load dataset\n",
    "train_df = a_DatasetCreation.download_data()\n",
    "train_df.columns = [\"Original\", \"Simplified\"]\n",
    "\n",
    "\n",
    "# Create conversations\n",
    "train_df = train_df.reset_index()\n",
    "gpt_conversation = train_df.apply(create_gpt_conversation, axis=1)\n",
    "davinci_conversation = train_df.apply(create_davinci_conversation, axis=1)\n",
    "\n",
    "# Save in json\n",
    "path = os.path.join(pwd, \"scratch\", \"text_simplification\", \"OpenAIFineTuning\")\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "gpt_conversation.to_csv(\n",
    "    os.path.join(path, \"gpt_conversation.json\"),\n",
    "    index=False,\n",
    "    header=False,\n",
    "    sep=\"\\n\",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    ")\n",
    "davinci_conversation.to_csv(\n",
    "    os.path.join(path, \"davinci_conversation.json\"),\n",
    "    index=False,\n",
    "    header=False,\n",
    "    sep=\"\\n\",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "We are now going to train the `GPT3.5` and `Davinci` models on the data generated by `GPT-4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to OpenAI API\n",
    "\n",
    "To simplify access to the OpenAI API, we're going to use the `openai` library to connect to the OpenAI API. Let's create a function so that we can easily connect to this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- CONNECT TO OPENAI ---------------------------- #\n",
    "import openai\n",
    "\n",
    "\n",
    "# Connect to OpenAI\n",
    "def connect_to_openai():\n",
    "    try:\n",
    "        with open(os.path.join(pwd, \".openai_key\"), \"r\") as f:\n",
    "            openai_key = f.read()\n",
    "            openai.api_key = openai_key\n",
    "    except:\n",
    "        key = input(\"Please enter your OpenAI key: \")\n",
    "        with open(os.path.join(pwd, \".openai_key\"), \"w\") as f:\n",
    "            f.write(key)\n",
    "        openai.api_key = key\n",
    "\n",
    "\n",
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_to_openai()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Fine-Tuning Function\n",
    "\n",
    "To make it easy to train models later on, let's turn it into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- FINE TUNE FUNCTION ---------------------------- #\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "def fine_tune_openai(file: str, model: str, save_path: str):\n",
    "    # Push training data to OpenAI\n",
    "    file = openai.File.create(\n",
    "        file=open(file),\n",
    "        purpose=\"fine-tune\",\n",
    "    )\n",
    "\n",
    "    # Wait for the file to be processed\n",
    "    logging.info(f\"Waiting for file {file['id']} to be processed\")\n",
    "    while file.status != \"processed\":\n",
    "        file = openai.File.retrieve(file.id)\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Fine tune model\n",
    "    logging.info(f\"Fine tuning {model} with {file['id']}\")\n",
    "    model = openai.FineTuningJob.create(\n",
    "        training_file=file[\"id\"],\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Wait for the fine tuning to be completed\n",
    "    while model.status != \"succeeded\":\n",
    "        model = openai.FineTuningJob.retrieve(model.id)\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Save model and file in one json\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    with open(os.path.join(save_path, f\"{model}_trained.json\"), \"w\") as f:\n",
    "        json.dump(\n",
    "            {\"model\": model, \"file\": file},\n",
    "            f,\n",
    "            indent=4,\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "\n",
    "    return model, file\n",
    "\n",
    "\n",
    "# import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune GPT-3.5 & Davinci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceUnavailableError",
     "evalue": "The server is overloaded or not ready yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServiceUnavailableError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ---------------------------- FINE TUNING DAVINCI --------------------------- #\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m davinci_model, davinci_file \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune_openai\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdavinci_conversation.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdavinci-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_simplification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOpenAIFineTuning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mfine_tune_openai\u001b[0;34m(file, model, save_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfine_tune_openai\u001b[39m(file: \u001b[38;5;28mstr\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m, save_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Push training data to OpenAI\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpurpose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfine-tune\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Wait for the file to be processed\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be processed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lingorank-llm-wa__1Cox-py3.9/lib/python3.9/site-packages/openai/api_resources/file.py:85\u001b[0m, in \u001b[0;36mFile.create\u001b[0;34m(cls, file, purpose, model, api_key, api_base, api_type, api_version, organization, user_provided_filename)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     user_provided_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m ):\n\u001b[1;32m     74\u001b[0m     requestor, url, files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__prepare_file_create(\n\u001b[1;32m     75\u001b[0m         file,\n\u001b[1;32m     76\u001b[0m         purpose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m         user_provided_filename,\n\u001b[1;32m     84\u001b[0m     )\n\u001b[0;32m---> 85\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mconvert_to_openai_object(\n\u001b[1;32m     87\u001b[0m         response, api_key, api_version, organization\n\u001b[1;32m     88\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lingorank-llm-wa__1Cox-py3.9/lib/python3.9/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    289\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lingorank-llm-wa__1Cox-py3.9/lib/python3.9/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    704\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    707\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/lingorank-llm-wa__1Cox-py3.9/lib/python3.9/site-packages/openai/api_requestor.py:755\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OpenAIResponse(\u001b[38;5;28;01mNone\u001b[39;00m, rheaders)\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m503\u001b[39m:\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mServiceUnavailableError(\n\u001b[1;32m    756\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe server is overloaded or not ready yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    757\u001b[0m         rbody,\n\u001b[1;32m    758\u001b[0m         rcode,\n\u001b[1;32m    759\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrheaders,\n\u001b[1;32m    760\u001b[0m     )\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext/plain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mServiceUnavailableError\u001b[0m: The server is overloaded or not ready yet."
     ]
    }
   ],
   "source": [
    "# ---------------------------- FINE TUNING DAVINCI --------------------------- #\n",
    "davinci_model, davinci_file = fine_tune_openai(\n",
    "    file=os.path.join(path, \"davinci_conversation.json\"),\n",
    "    model=\"davinci-002\",\n",
    "    save_path=os.path.join(pwd, \"results\", \"text_simplification\", \"OpenAIFineTuning\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for file file-Yxm1w5GG8MMmXA4QDviKc27l to be processed\n",
      "INFO:root:Fine tuning gpt-3.5-turbo-1106 with file-Yxm1w5GG8MMmXA4QDviKc27l\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- FINE TUNING DAVINCI --------------------------- #\n",
    "davinci_model, davinci_file = fine_tune_openai(\n",
    "    file=os.path.join(path, \"gpt_conversation.json\"),\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    save_path=os.path.join(pwd, \"results\", \"text_simplification\", \"OpenAIFineTuning\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
