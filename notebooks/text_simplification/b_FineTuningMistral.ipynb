{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Mistral\n",
    "\n",
    "Thanks to [our first notebook](a_DatasetCreation.ipynb), we now have a training dataset containing *30 sentences* at each level ${A2, B1, B2, C1, C2}$ and their simplified versions at level ${A1, A2, B1, B2, C1}$. We will now fine-tune a version of **Mistral** specially designed for French on this dataset in order to have a model capable of simplifying French sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- PREPARING NOTEBOOK ---------------------------- #\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Random seed\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# External modules\n",
    "import os\n",
    "\n",
    "# Set global log level\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Define PWD as the current git repository\n",
    "import git\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "pwd = repo.working_dir\n",
    "os.chdir(pwd)\n",
    "\n",
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- LOAD PREVIOUS NOTEBOOKS ------------------------- #\n",
    "import json\n",
    "import __main__\n",
    "import black\n",
    "\n",
    "paths = [\n",
    "    os.path.join(pwd, \"notebooks\", \"text_simplification\", \"a_DatasetCreation.ipynb\"),\n",
    "]\n",
    "\n",
    "# Read notebooks\n",
    "code_dict = {}\n",
    "for path in paths:\n",
    "    code = \"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        temp = json.load(f)\n",
    "\n",
    "    cells = [\n",
    "        cell\n",
    "        for cell in temp[\"cells\"]\n",
    "        if cell[\"cell_type\"] == \"code\"\n",
    "        and len(cell[\"source\"]) > 0\n",
    "        and cell[\"source\"][-1] == \"# import\"\n",
    "    ]\n",
    "    notebook_code = \"\\n\".join(\n",
    "        line\n",
    "        for cell in cells\n",
    "        for line in cell[\"source\"]\n",
    "        if line != \"# import\" and len(line) > 0 and line[0] != \"%\"\n",
    "    )\n",
    "    # Create something like a header\n",
    "    code += f\"# {'-'*76} #\\n\"\n",
    "    code += f\"# {os.path.basename(path).upper():^76} #\\n\"\n",
    "    code += f\"# {'-'*76} #\\n\"\n",
    "    code += notebook_code\n",
    "\n",
    "    # Add \"Module Creation\"\n",
    "    notebook_name = (\n",
    "        os.path.basename(path).replace(\"imported_\", \"\").replace(\".ipynb\", \"\")\n",
    "    )\n",
    "    code += \"\"\"\n",
    "# --------------------------------- IMPORTER --------------------------------- #\n",
    "import types\n",
    "\n",
    "\n",
    "class MyNotebook:\n",
    "    pass\n",
    "\n",
    "\n",
    "NOTEBOOK_NAME = MyNotebook()\n",
    "# Put every function defined in the notebook in the class\n",
    "NOTEBOOK_NAME.__dict__.update(\n",
    "    {\n",
    "        name: obj\n",
    "        for name, obj in locals().items()\n",
    "        if isinstance(obj, (type, types.FunctionType))\n",
    "        if not (name.startswith(\"_\") or name == \"MyNotebook\")\n",
    "    }\n",
    ")\n",
    "    \"\"\".replace(\n",
    "        \"NOTEBOOK_NAME\", notebook_name\n",
    "    )\n",
    "\n",
    "    # Remove empty lines\n",
    "    code = \"\\n\".join([line for line in code.split(\"\\n\") if len(line) > 0])\n",
    "    # Format code\n",
    "    code = black.format_str(code, mode=black.FileMode())\n",
    "\n",
    "    # Write scrach file\n",
    "    path = os.path.join(\n",
    "        pwd, \"scratch\", f\"imported_{os.path.basename(path).replace('ipynb', 'py')}\"\n",
    "    )\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        os.makedirs(os.path.dirname(path))\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(code)\n",
    "    code_dict[path] = code\n",
    "\n",
    "\n",
    "# Mainify code\n",
    "for path, code in code_dict.items():\n",
    "    compiled = compile(code, path, \"exec\")\n",
    "    exec(compiled, __main__.__dict__)\n",
    "\n",
    "# import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "We create a function to load the **Mistral-7B** (*Vigostral*) model by applying a **LoRa** configuration to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "def load_model(model_name : str):\n",
    "    # Load model\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, device_map=\"auto\", use_cache=False\n",
    "        )\n",
    "    except:\n",
    "        if torch.cuda.is_available():\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, device_map=\"cuda\", use_cache=False\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, device_map=\"cpu\", use_cache=False\n",
    "            )\n",
    "\n",
    "    # Configure model\n",
    "    config = LoraConfig(\n",
    "        r=64,  # Plus r est grand, plus le modèle est précis mais plus il est lent\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "            \"lm_head\",\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=0.05,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, config)\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "# import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Fine-Tuning function\n",
    "\n",
    "First we're going to create the function that will run on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- FINE-TUNING FUNCTION --------------------------- #\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "MODEL = \"bofenghuang/vigostral-7b-chat\"\n",
    "\n",
    "def train_mistral(\n",
    "    pwd: str = \".\",\n",
    "    use_ray: bool = False,\n",
    "    max_training_epochs: int = 20,\n",
    "):\n",
    "    # Fix partial import bug\n",
    "    import ray.train.huggingface\n",
    "    import ray.train.huggingface.transformers\n",
    "\n",
    "    # Load data\n",
    "    df = a_DatasetCreation.download_data(pwd=pwd)\n",
    "\n",
    "    # Charger tokenizer\n",
    "    tokenizer = a_DatasetCreation.download_tokenizer()\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = a_DatasetCreation.format_data(df, tokenizer, training=True)\n",
    "\n",
    "    # Encode dataset\n",
    "    encoded_dataset = a_DatasetCreation.encode_dataset(dataset, tokenizer)\n",
    "\n",
    "    # Create train and validation dataset\n",
    "    split = encoded_dataset.train_test_split(test_size=0.25, shuffle=False)\n",
    "    train_dataset = split[\"train\"]\n",
    "    validation_dataset = split[\"test\"]\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(MODEL)\n",
    "\n",
    "    # Create model folder if it doesn't exist\n",
    "    path = os.path.join(\n",
    "        pwd,\n",
    "        \"models\",\n",
    "        \"difficulty_estimation\",\n",
    "        MODEL.replace(\"/\", \"_\"),\n",
    "    )\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # Configure WandB\n",
    "    os.environ[\"WANDB_PROJECT\"] = \"mistral_sentence_simplification\"\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "    # Créer le Trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=path,\n",
    "        warmup_steps=1,\n",
    "        num_train_epochs=max_training_epochs,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=2.5e-5,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,\n",
    "        logging_dir=os.path.join(path, \"logs\"),\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        do_eval=True,\n",
    "        load_best_model_at_end=True,\n",
    "        run_name=f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n",
    "        ddp_find_unused_parameters=False,  # Necessary for FSDP to work\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,  # le modèle à entraîner\n",
    "        args=training_args,  # les arguments d'entraînement\n",
    "        train_dataset=train_dataset,  # le jeu de données d'entraînement\n",
    "        eval_dataset=validation_dataset,  # le jeu de données d'évaluation\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    if use_ray:\n",
    "        # Add ray tune callback\n",
    "        callback = ray.train.huggingface.transformers.RayTrainReportCallback()\n",
    "        trainer.add_callback(callback)\n",
    "        trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    result = trainer.train()\n",
    "\n",
    "    # Write checkpoint\n",
    "    checkpoint_path = os.path.join(path, \"mistral_simplification_trained\")\n",
    "    trainer.save_model(checkpoint_path)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Execution function\n",
    "\n",
    "We now need to create the function that Slurmray executes on the cluster. We need a way to start Ray correctly and retrieve the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ CLUSTER EXECUTION FUNCTION ------------------------ #\n",
    "from ray.train import ScalingConfig, CheckpointConfig, RunConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "\n",
    "def ray_launcher(config):\n",
    "    # Get config\n",
    "    kwargs = config[\"kwargs\"]\n",
    "    f = config[\"f\"]\n",
    "\n",
    "    # Run function\n",
    "    return f(**kwargs)\n",
    "\n",
    "\n",
    "def slurmray_function(\n",
    "    pwd: str = \"/scratch/hjamet\",\n",
    "    max_training_epochs: int = 20,\n",
    "):\n",
    "    # Create ray launcher\n",
    "    ray_trainer = TorchTrainer(\n",
    "        ray_launcher,\n",
    "        scaling_config=ScalingConfig(num_workers=1, use_gpu=True),\n",
    "        run_config=RunConfig(\n",
    "            checkpoint_config=CheckpointConfig(num_to_keep=1),\n",
    "            storage_path=pwd,\n",
    "        ),\n",
    "        train_loop_config={\n",
    "            \"f\": train_mistral,\n",
    "            \"kwargs\": {\n",
    "                \"pwd\": pwd,\n",
    "                \"use_ray\": True,\n",
    "                \"max_training_epochs\": max_training_epochs,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    result = ray_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Launcher\n",
    "\n",
    "Now that we've perfectly defined the code to be run on the cluster, we can create a function that will allow us to launch the training. To do this, we're going to use the **Slurmray** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- SLURMRAY LAUNCHER ---------------------------- #\n",
    "from slurmray.RayLauncher import RayLauncher\n",
    "\n",
    "launcher = RayLauncher(\n",
    "    project_name=\"mistral_sentence_simplification\",\n",
    "    func=slurmray_function,\n",
    "    args={\n",
    "        \"max_training_epochs\": 20,\n",
    "    },\n",
    "    modules=[\"cuda/11.8.0\"],\n",
    "    node_nbr=1,\n",
    "    use_gpu=True,\n",
    "    memory=128,\n",
    "    max_running_time=60 * 2,\n",
    "    server_run=True,\n",
    "    server_ssh=\"curnagl.dcsr.unil.ch\",\n",
    "    server_username=\"hjamet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the training\n",
    "\n",
    "We can now launch the training on the cluster using our previously defined function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = launcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
